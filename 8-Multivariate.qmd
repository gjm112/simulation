---
title: "Multivariate"
author: "Gregory J. Matthews"
format: 
  revealjs:
    chalkboard: true
    slide-number: c/t
    code-line-numbers: false
    linestretch: 1.25
    html-math-method:
      method: mathjax
      url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
editor: visual
execute: 
  echo: true
---

## Multivariate Simulation

-   Copula Models
-   MCMC
    -   Metropolis Hastings
    -   Gibbs Sampling

## Basic Idea

-   Want to be able to sample from a target distribution.
-   The distribution now is multivariate.\
-   The variables will now have a correlation structure

## Multivariate normal distribution

-   ${\bf x} = (x_1,x_2)$
-   pdf: $f({\bf x}) = \frac{1}{(2\pi|{\bf \Sigma}|)^{\frac{k}{2}}}e^{(-\frac{1}{2}({\bf x}-{\bf \mu})'{\bf \Sigma}^{-1}({\bf x}-{\bf\mu}))}$
-   Mean vector: $\mu = (\mu_1, \mu_2)$.
-   Correlation matrix: $$\Sigma = 
       \begin{bmatrix}
      \sigma^2_1 & \sigma_{12} \\
      \sigma_{12} & \sigma^2_2 \\
      \end{bmatrix}$$.

## Multivariate normal distribution: Bivariate example

```{r}
library(tidyverse)
library(mvtnorm)
dat <- expand.grid(seq(-3,3,.1),seq(-3,3,0.1))
dat <- cbind(dat, dmvnorm(dat))
names(dat) <- c("x","y","z")
ggplot(aes(x = x, y = y, z = z), data = dat) + geom_contour_filled() + coord_fixed()
```

## Multivariate normal distribution: Bivariate example

```{r}
library(mvtnorm)
dat <- expand.grid(seq(-3,3,.1),seq(-3,3,0.1))
dat <- cbind(dat, dmvnorm(dat, mean = c(0,0), sigma = matrix(c(1,.5,.5,1), ncol = 2)))
names(dat) <- c("x","y","z")
ggplot(aes(x = x, y = y, z = z), data = dat) + geom_contour_filled() + coord_fixed()
```

## Multivariate normal distribution: Bivariate example

```{r}
library(mvtnorm)
dat <- expand.grid(seq(-3,3,.1),seq(-3,3,0.1))
dat <- cbind(dat, dmvnorm(dat, mean = c(0,0), sigma = matrix(c(1,-.5,-.5,1), ncol = 2)))
names(dat) <- c("x","y","z")
ggplot(aes(x = x, y = y, z = z), data = dat) + geom_contour_filled() + coord_fixed()
```

## Properties of MVN

-   What is the conditional distribution of $X_1 | X_2$?
-   $X_1 | X_2 = x_2 \sim N(\mu^\star, \Sigma^\star)$
    -   $\mu^\star = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(x_2 - \mu_2)$
    -   $\Sigma^\star = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}$
-   Bivariate case:
    -   $\mu^\star = \mu_1 + \frac{\sigma_{12}}{\sigma^2_{2}}(x_2 - \mu_2)$
    -   $\sigma^\star = \sigma^2_{1} - \frac{\sigma_{12}}{\sigma^2_{2}}$

## Randomly sampling from bivariate normal

-   Recall, $p(A \cap B) = p(A|B)p(B)$.\
-   So if we want to sample from $f(x_1,x_2)$ we can instead sample from $f(x_2)$ first and then sample from $f(x_1|x_2)$.

## Randomly sampling from bivariate normal

```{r}
#target distribution
mu <- c(3,5)
Sigma <- matrix(c(10,6,6,10), ncol = 2)
Sigma

nsim <- 1000
#Marginal distribution of x2 is N(5,10)
x2 <- rnorm(nsim, mu[2], Sigma[2,2])

#Conditional distribution of x1|x2 is N(mu_star, sigma_star)
mu_star <- mu[1] + Sigma[1,2]/Sigma[2,2]*(x2 - mu[2])
sigma_star <- Sigma[1,1] - Sigma[1,2]/Sigma[2,2]
x1 <- rnorm(nsim, mu_star, sigma_star) 
dat <- data.frame(x1,x2)
```

## Randomly sampling from bivariate normal

```{r}
library(ggExtra)
p <- ggplot(aes(x = x1, y= x2),data = dat) + geom_point() + theme_bw() + coord_fixed()

# use ggMarginal function to create marginal histogram
ggMarginal(p, type="histogram")
```

## Copula

-   Consider vector $(X_1, X_2, \cdots, X_d)$.
-   Each $X_i$ has a continuous cdf $F_{X_i}(x)$.
-   $(U_1, U_2, \cdots, U_d) = (F_{X_1}(X_1), F_{X_2}(X_2), \cdots, F_{X_d}(X_d)$
-   The marginals a uniform.\
-   We say that $(U_1, U_2, \cdots, U_d)$ is the copula of $(X_1, X_2, \cdots, X_d)$.
-   The copula contains the information about the correlation structure, and infromation about the marginal distributions are contained in $F_{X_i}(x)$.

## Inverting the Copula

-   If we reverse these steps, this gives us a way of generating pseudo-random numbers from multivariate distributions.\
-   $(X_1, X_2, \cdots, X_d)$ = $(F^{-1}_{X_1}(U_1),F^{-1}_{X_1}(U_2),\cdots, F^{-1}_{U_d}(x))$
-   So it we can generate $(U_1, U_2, \cdots, U_d)$, then we can generate $(X_1, X_2, \cdots, X_d)$.
-   So how do we generate $(U_1, U_2, \cdots, U_d)$?

## The Gaussian Copula

-   $C(u) = \Phi_R(\Phi^{-1}(u_1),\Phi^{-1}(u_2),\cdots,\Phi^{-1}(u_d))$
    -   $R$ is a correlation structure.\
    -   $\Phi$ is the normal cdf.

## Copula Example

-   Let's try to sample from a bivariate gamma distribution with large positive correlation.\

```{r}
library(mvtnorm)
mu <- c(0,0)
Sigma <- matrix(c(1,0.5, 0.5, 1), ncol = 2)
nsim <- 10000
x <- rmvnorm(nsim, mu, Sigma)
#Transform to Uniform
x[,1] <- pnorm(x[,1])
x[,2] <- pnorm(x[,2])
```

## Copula Example

```{r}
library(ggExtra)
p <- ggplot(aes(x = V1, y = V2),data = as.data.frame(x)) + geom_point() + theme_bw() + coord_fixed() + theme(legend.position="none")

# use ggMarginal function to create marginal histogram
ggMarginal(p, type="histogram")
```

## Copula Example

```{r}
#Now we take the copula to the marginals that we want.  
x[,1] <- qgamma(x[,1],3,5)
x[,2] <- qgamma(x[,2],1,6)
cor(x)

library(ggExtra)
p <- ggplot(aes(x = V1, y = V2),data = as.data.frame(x)) + geom_point() + theme_bw() + coord_fixed() + theme(legend.position="none")

# use ggMarginal function to create marginal histogram
ggMarginal(p, type="histogram")
```

## Copula Example: Correlated Binomial

```{r}
library(mvtnorm)
mu <- c(0,0)
Sigma <- matrix(c(1,0.5, 0.5, 1), ncol = 2)
nsim <- 10000
x <- rmvnorm(nsim, mu, Sigma)
#Transform to Uniform
x[,1] <- pnorm(x[,1])
x[,2] <- pnorm(x[,2])

x[,1] <- qbinom(x[,1],1,0.6)
x[,2] <- qbinom(x[,2],1,0.6)
#Doesn't maintain the exact corelation because you are moving to discrete.  But still correlated.  
cor(x)
```

## R copula package

```{r}
library(copula)
nsims <- 1000
x <- rCopula(nsims, copula = normalCopula(0.5, dim = 2, dispstr = "un"))
library(ggExtra)
p <- ggplot(aes(x = V1, y = V2),data = as.data.frame(x)) + geom_point() + theme_bw() + coord_fixed() + theme(legend.position="none")

# use ggMarginal function to create marginal histogram
ggMarginal(p, type="histogram")
cor(x)
  
```

## R copula package: Kendall

```{r}
library(copula)
nsims <- 10000
#First calibrate with iTau to get a target Kendall's tau distribution. 
theta <- iTau(normalCopula(), tau = c(.5))
x <- rCopula(nsims, copula = normalCopula(theta, dim = 2, dispstr = "un"))
plot(x)
cor(x, method = "kendall")
cor(qbinom(x,1,0.5), method = "kendall")
```

## R copula package

```{r}
library(copula)
nsims <- 1000
x <- rCopula(nsims, copula = normalCopula(c(0.5,0.5,0.5), dim = 3, dispstr = "un"))
pairs(x)
cor(x)
```

## R copula package

```{r}
library(copula)
nsims <- 1000
x <- rCopula(nsims, copula = normalCopula(c(0.5,0.5,0.5,0.5,0.5,0.5), dim = 4, dispstr = "un"))
pairs(x)
cor(x)
```

## Copula Application

[Olympic Sport Climbing](https://jds-online.org/journal/JDS/article/1273/info)

## Simulation Study

```{r}
ec <- read.csv("/Users/gregorymatthews/Dropbox/simulationgit/2024_Electoral_College.csv")


nsim <- 10000
ecsim <- c()
for (i in 1:nsim){
#Generate random bernoulli's
ecsim[i] <- sum(rbinom(51,1,ec$Prob)*ec$Total)
}

mean(ecsim)
median(ecsim)
barplot(ecsim)

sum(ecsim > 270)/nsim
sum(ecsim < 270 )/nsim
sum(ecsim == 269)/nsim

#Now add correlation 
library(tidyverse)
swing <- ec %>% filter(Prob == 0.5)
notswing <- ec %>% filter(Prob != 0.5)


theta <- iTau(normalCopula(), tau = rep(.7,21))
x <- rCopula(nsim, copula = normalCopula(theta, dim = 7, dispstr = "un"))
cor(x, method = "kendall")


ecsimwcor <- apply(x,1,function(z){sum(qbinom(z,1,swing$Prob)*swing$Total)}) + sum(notswing$Prob*notswing$Total)


sum(ecsimwcor > 270)/nsim
sum(ecsimwcor < 270 )/nsim
sum(ecsimwcor == 269)/nsim












```

-   Write a simulation to simulate the electoral college in 2 ways:

1.  All states are independent.\
2.  There is correlation between the 7 swing states.

## MCMC

-   What is MCMC?
    -   Markov Chain Monte Carlo?
-   What is Monte Carlo?
    -   Simulation based study.
-   What is a Markov Chain?
    -   A chain that only depends on the most recent state.

## Monte Carlo

-   Monte Carlo techniques solve problems by simulation
-   Simple example:
    -   Say I have a fair coin and I want to know the probability that when the coin is flipped it will land on heads.\
    -   Since there are two events in the sample space $\{H,T\}$, and I am interested in one of those events $\{H\}$, the probability will be $\frac{1}{2}$.
    -   However, I could also solve this problem via Monte Carlo simulation.\
    -   How? Get out a coin, flip it a large number of times, and record the results.\
    -   That's it!
    -   FERMIAC

## What is a Markov Chain?

-   A Markov chain is a sequence of random variable where the current random variable is only effected by the most recent random variable and is independent of the all others.

-   Formally, $$
     Pr(X_{n+1}=x|X_{n}=x_{n},X_{n-1}=x_{n-1},....,X_{0}=x_{0})
     $$ $$
     =Pr(X_{n}=x|X_{n-1}=x_{n-1})
     $$

-   Basically, if I know the state I am in at time $n-1$, the probability of moving to state $x$ at time $n$ is the same whether or not I have information about the state I was in at times $0$ through $n-1$

## What is a Markov Chain?

-   We need two things for a Markov Chain:
    1.  Initial probability matrix: The probability of starting in any given state
    2.  Transition matrix: For any given state, this matrix gives the probability of moving to the next state.

## An example

-   The Drunkards walk.\

```{r}
set.seed(1234)
x<-c()
x[1]<-0
for (i in 2:25){
x[i]<-x[i-1]+sign(rnorm(1,0,1))
}
x
```

-   Question: What would the transition matrix look like for this Markov chain?

## Metropolis-Hastings

-   We need:
    -   

        1.  target distribution $\pi$ and 2. transition kernel $Q$ (generates proposals).
-   Initialize $X_1 = x_1$
-   For t = $1, 2, \cdots$,
    -   Sample $x'$ from $Q(x'|x_t)$.\
    -   Compute $A = min\left(1,\frac{\pi(x')Q(x_t|x')}{\pi(x_t)Q(x'|x_t)}\right)$ (A is the "acceptance probability".)
    -   Accept $x'$ with probability A and set $x_{t+1} = x'$. Otherwise $x_{t+1} = x_t$.

## Metropolis-Hastings

-   Let's implement this for a bivariate normal distribution.
-   Let $\mu = c(10,20)$.
-   Let $\Sigma =
    \begin{bmatrix}
      15 & 5 \\
      5 & 10 \\
      \end{bmatrix}$.

## Metropolis-Hastings

```{r}
#Let's implement this. 
library(mvtnorm)
mu <- c(10,20)
Sigma <- matrix(c(15,5,5,10), ncol = 2)
#Initialize x
nsim <- 10000
x <- matrix(c(0,0), ncol = 2, nrow = nsim)
for (t in 1:(nsim-1)){
#Propose an x'
xprime <- c(NA, NA)
xprime[1] <- rnorm(1,x[t,1],1)
xprime[2] <- rnorm(1,x[t,2],1)

#Compute A
ratio1 <- dmvnorm(xprime,mu,Sigma)/dmvnorm(x[t,],mu,Sigma)
ratio2 <- dmvnorm(x[t,],mean = xprime)/dmvnorm(xprime, mean = x[t,])
A <- min(1,ratio1*ratio2)

if (runif(1) < A){x[t+1,] <- xprime} else {x[t+1,] <- x[t,]}
}

```

## Metrpolis-Hastings

```{r}
library(tidyverse)
dat <- expand.grid(seq(-5,20,.5),seq(0,30,0.5))
dat <- cbind(dat, dmvnorm(dat, mu, Sigma))
names(dat) <- c("x","y","z")
ggplot(aes(x = V1, y = V2), data = as.data.frame(x)) + geom_path() + geom_contour(aes(x = x, y = y, z = z), data = dat) + coord_fixed()
```

## Burn-in and thinning

-   Burn-in: We throw away the first $n$ draws to prevent us including draws from before convergence of the chain.
-   Thinning: We take only every $t$-th iteration. We want an independent sample, but consecutive draws are often highly correlated.

## Metrpolis-Hastings

```{r}
#Now drop the first say 1000.  
library(tidyverse)
dat <- expand.grid(seq(-5,20,.5),seq(0,30,0.5))
dat <- cbind(dat, dmvnorm(dat, mu, Sigma))
names(dat) <- c("x","y","z")
ggplot(aes(x = V1, y = V2), data = as.data.frame(x[1000:10000,])) + geom_point() + geom_contour(aes(x = x, y = y, z = z), data = dat) + coord_fixed()
```

## Metropolis-Hastings

```{r}
acf(x[,1])
acf(x[,2])
```

## Pros and Cons

-   Cons:
    -   Samples are autocorelated. Effective sample sizes can be lower.\
    -   Need a "burn-in" period for algorithm to converge.
-   Pros:
    -   Does not suffer from the "curse of dimensionality" like acceptance-rejection algorithms that generate independent samples. This is why this is often used in high dimensional Bayesian models.

## Gibb's Sampling

-   We will sample from a joint distribution by iteratively sampling from all of the conditional distributions.\
-   Consider $P(X_1, X_2, X_3)$. We want to sample from it.\
-   We want to create a chain of values $X_{i,t}$
-   Start by initializing values for $X_{1,0}, X_{2,0}, X_{3,0}$.
-   Then sample iteratively from conditionals:
    -   Draw $X_{1,1}$ from \$P(X\_{1} \| X\_{2,0}, X\_{3,0})
    -   Draw $X_{2,1}$ from \$P(X\_{2} \| X\_{1,1}, X\_{3,0})
    -   Draw $X_{3,1}$ from \$P(X\_{3} \| X\_{1,1}, X\_{2,1})
-   Repeat.

## Bivariate Gibb's Example

-   Let's implement this for a bivariate normal distribution.
-   Let $\mu = c(10,20)$.
-   Let $\Sigma =
       \begin{bmatrix}
      15 & 5 \\
      5 & 10 \\
      \end{bmatrix}$.

## Bivariate Gibb's Example

```{r}
library(mvtnorm)
mu <- c(10,20)
Sigma <- matrix(c(15,5,5,10), ncol = 2)
#Initialize x
nsim <- 10000
x <- matrix(c(-100,-100), ncol = 2, nrow = nsim)

for (t in 1:(nsim-1)){
#X1|X2
mu12 <- mu[1] + Sigma[1,2]/Sigma[2,2]*(x[t,2] - mu[2])
sigma12 <- Sigma[1,1] - Sigma[1,2]*Sigma[2,1]/Sigma[2,2]
x[t+1,1] <- rnorm(1,mu12, sqrt(sigma12))

#X2|X1
mu21 <- mu[2] + Sigma[1,2]/Sigma[1,1]*(x[t+1,1] - mu[1])
sigma21 <- Sigma[2,2] - Sigma[1,2]*Sigma[2,1]/Sigma[1,1]
x[t+1,2] <- rnorm(1,mu21, sqrt(sigma21))
}
```

## Bivariate Gibb's Example

```{r}
library(tidyverse)
dat <- expand.grid(seq(-5,20,.5),seq(0,30,0.5))
dat <- cbind(dat, dmvnorm(dat, mu, Sigma))
names(dat) <- c("x","y","z")
ggplot(aes(x = V1, y = V2), data = as.data.frame(x)) + geom_path() + geom_contour(aes(x = x, y = y, z = z), data = dat) + coord_fixed()
```

## Bivariate Gibb's Example

```{r}
acf(x[,1])
acf(x[,2])
```

## An even simpler example of Gibb's Sampling

-   Say we have some discrete joint distribution $P(X,Y)$.
-   Where both $X$ and $Y$ are binary random variables.
-   We can define the joint distribution with a contingency table:

```{r echo = FALSE}
tab <- matrix(c(0.6,0.1,0.1,0.2), ncol = 2)
row.names(tab) <- c("X = 1","X = 0")
colnames(tab) <- c("Y = 1","Y = 0")
tab
```

## An even simpler example of Gibb's Sampling

```{r echo = FALSE}
tab <- matrix(c(0.6,0.1,0.1,0.2), ncol = 2)
row.names(tab) <- c("X = 1","X = 0")
colnames(tab) <- c("Y = 1","Y = 0")
tab
```

-   Find:
    -   P(X = 1 \| Y = 1)
    -   P(X = 1 \| Y = 0)
    -   P(Y = 1 \| X = 1)
    -   P(Y = 1 \| X = 0)
-   We have all the conditionals!

## An even simpler example of Gibb's Sampling

```{r}
#Now let's sample from this. 
nsim <- 1000
x <- y <- rep(NA, nsim)
x[1] <- y[1] <- 0

set.seed(666)
#Write this here
for (i in 1:(nsim -1)){
px <- ifelse(y[i] == 1, 6/7, 1/3)
x[i+1] <- rbinom(1, 1, px)

py <-  ifelse(x[i+1] == 1, 6/7, 1/3)
y[i+1] <- rbinom(1, 1, py)
}






```

## Bayesian

-   Ok. Ready?\
-   There are two main schools of thought in statistics:
    -   Frequentist
    -   ***Bayesian***

## Bayesian vs Frequentist

-   Frequentist
    -   Parameters are fixed and unknown.
    -   Parameters are single points.
    -   We estimate parameters with point estimates and confidence intervals.\
    -   Everything you've probably ever done.

## Bayesian vs Frequentist

-   Bayesian
    -   Parameters are considered to be random variables.
    -   We estimate parameters with distributions called posterior distributions.\
    -   Credible intervals and distribution summaries (e.g. posterior means).

## Bayesian

-   So what is Bayes Rule? $$
       P(A|B) = \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A^c)P(A^c)}
       $$
-   Can we derive this? Yes. Let's do it.

## Bayesian

-   So how can we use Bayes rule to do Bayesian statistics?
-   Let's say we have a parameter $\mu$ (the classic populatio mean).
-   I want to learn about it so I take a sample form the population. Let's call the sample $x$.
-   What I want to know about is this: $$
      P(\mu|x)
      $$
-   This will tell us the distribution of $\mu$ given the data that we collected.\
-   How do we get this!? (And where is Bayes?!)

## Bayesian

-   Let's put this in terms of Bayes rule: $$
      P(\mu|x) = \frac{P(x|\mu)P(\mu)}{P(x)}
      $$
-   The denominator is just a constant so we can re-write this as: $$
      P(\mu|x) \propto P(x|\mu)P(\mu)
      $$

## Bayesian

-   P($\mu$): Prior
-   Likelihood: P(x\|$\mu$)
-   Posterior: P($\mu$\|x)

## Simplest Bayesian Example: Binomial

-   Likelihood: $X|p \sim$ Binomial(n,p) ($n$ assumed fixed)
-   Prior $p \sim$ Beta($\alpha$,$\beta$).

## Let's do some math!

$$
P(p|x) \propto P(x|p)P(p)
$$ $$
 = {n \choose x} p^x (1-p)^{n-x} \frac{1}{\beta(\alpha,\beta)}p^{\alpha - 1}(1-p)^{beta - 1}
$$ $$
\propto p^x (1-p)^{n-x} p^{\alpha - 1}(1-p)^{\beta - 1}
$$

$$
 p^{x + \alpha + 1} (1-p)^{n-x+\beta-1}
$$ - This is a Beta($\alpha^{\star} = x + \alpha, \beta^{\star} = n - x + \beta$) - $E[p|x] = \frac{x + \alpha}{n + \alpha + \beta}$

## Data Example

```{r}
#Play around we these parameters
x <- 2
n <- 2
alpha <- 5
beta <- 5

#Posterior
xxx <- seq(0,1,0.01)
yyy <- dbeta(xxx,x + alpha,n - x + beta)
plot(xxx,yyy,type = "l", col = "red")
#Priot
yyy <- dbeta(xxx,alpha,beta)
points(xxx,yyy,type = "l")

```

## Bayesian with STAN

-   $y|\theta \sim$ binomial(n,$\theta$)
-   $\theta \sim$ Beta($\alpha,\beta$)

## Bayesian with STAN

```{r}
data <- list(y = 2, n = 2)

library(rstan)
fit1 <- stan(
  file = "example.stan",  # Stan program
  data = data,    # named list of data
  chains = 4,             # number of Markov chains
  warmup = 1000,          # number of warmup iterations per chain
  iter = 1000 + 1000*3,            # total number of iterations per chain
  cores = 1,              # number of cores (could use one per chain)
  refresh = 0, 
  thin = 3 # no progress shown
  )
```

## Bayesian with STAN

```{r}
#Results summary
fit1
plot(fit1)
```

## Bayesian with STAN

```{r}
#Checking convergence
traceplot(fit1)
```

## Bayesian with STAN

```{r}
#Posterior Distribution
library(bayesplot)
mcmc_areas(fit1,
           pars = c("theta"),
           prob = 0.8) 
```

## Bayesian with STAN

```{r}
#Checking convergence
posterior2 <- extract(fit1, inc_warmup = TRUE, permuted = FALSE)

color_scheme_set("mix-blue-pink")
p <- mcmc_trace(posterior2,  pars = c("theta"), n_warmup = 1000,
                facet_args = list(nrow = 2, labeller = label_parsed))
p
```

## Bayesian with STAN with a strong prior

```{r}
data <- list(y = 2, n = 2)

library(rstan)
fit2 <- stan(
  file = "example_informative_prior.stan",  # Stan program
  data = data,    # named list of data
  chains = 4,             # number of Markov chains
  warmup = 1000,          # number of warmup iterations per chain
  iter = 2000,            # total number of iterations per chain
  cores = 1,              # number of cores (could use one per chain)
  refresh = 0             # no progress shown
  )
```

## Bayesian with STAN

```{r}
#Results summary
fit2
plot(fit2)
```

## Bayesian with STAN

```{r}
#Checking convergence
traceplot(fit2)
```

## Bayesian with STAN

```{r}
#Posterior distribution
library(bayesplot)
mcmc_areas(fit2,
           pars = c("theta"),
           prob = 0.8) 
```

## Bayesian with STAN

```{r}
#Checking convergence
posterior2 <- extract(fit2, inc_warmup = TRUE, permuted = FALSE)

color_scheme_set("mix-blue-pink")
p <- mcmc_trace(posterior2,  pars = c("theta"), n_warmup = 1000,
                facet_args = list(nrow = 2, labeller = label_parsed))
p
```

## Write STAN code to fit the following model

-   $y|\mu,\sigma^2 ~ N(\mu,\sigma^2)$
-   $p(\mu,\sigma^2) \propto 1$
-   Note: STAN uses SD
-   Data = 13,-1,8,-12,9,-15,3,27,-4,10

## Code

```{r}
data <- list(y = c(13,-1,8,-12,9,-15,3,27,-4,10), n = 10)

library(rstan)
fit3 <- stan(
  file = "haidavai.stan",  # Stan program
  data = data,    # named list of data
  chains = 4,             # number of Markov chains
  warmup = 1000,          # number of warmup iterations per chain
  iter = 2000,            # total number of iterations per chain
  cores = 1,              # number of cores (could use one per chain)
  refresh = 0             # no progress shown
  )
```

## Bayesian with STAN

```{r}
#Results summary
fit3
plot(fit3)
```

## Bayesian with STAN

```{r}
#Checking convergence
traceplot(fit2)
```

## Bayesian with STAN

```{r}
#Posterior distribution
library(bayesplot)
mcmc_areas(fit3,
           pars = c("mu","sigma"),
           prob = 0.8) 
```

## Bayesian with STAN

```{r}
#Checking convergence
posterior2 <- extract(fit3, inc_warmup = TRUE, permuted = FALSE)

color_scheme_set("mix-blue-pink")
p <- mcmc_trace(posterior2,  pars = c("mu","sigma"), n_warmup = 1000,
                facet_args = list(nrow = 2))
p
```

## Bayesian with brms

```{r}
#| eval: false
library(brms)
house <- read.csv("./house_price_regression_dataset.csv")

fit_house <- brm(House_Price ~ Square_Footage + Num_Bedrooms + Num_Bathrooms + Lot_Size, data = house)
```

```{r}
library(brms)
load("./fit_house.RData")
```

## Bayesian with brms

```{r}
fit_house
```

## Bayesian with brms

```{r}
plot(fit_house, variable = "Intercept")
```

## Bayesian with brms

```{r}
plot(fit_house, variable = c("b_Square_Footage","b_Num_Bedrooms","b_Num_Bathrooms","b_Lot_Size"))
```

## Bayesian with brms

```{r}
library(bayesplot)
mcmc_areas(fit_house,
           pars = c("b_Intercept","b_Num_Bedrooms","b_Num_Bathrooms","b_Lot_Size"),
           prob = 0.8) 

```

## Bayesian with brms

```{r}
library(bayesplot)
mcmc_areas(fit_house,
           pars = c("b_Square_Footage"),
           prob = 0.8) 

```

## Bayesian with brms

```{r}
stancode(fit_house)
```
